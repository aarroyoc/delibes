Ya hemos visto con anterioridad que existen modelos que nos hacen la vida más sencilla. Sin embargo, en esos modelos conocíamos ciertos datos de antemano. ¿Qué pasa si tenemos datos y queremos ver si podemos estar ante un modelo de los ya definidos?<br><br>Este tipo de ajuste es muy interesante, ya que nos permite saber si los datos en bruto pueden parecerse a los modelos de <strong>Normal</strong> u otros y aprovecharlo.<br><h2>Ajuste de datos a una distribución</h2><br>Para ajustar datos a una distribución, todas las distribuciones continuas de SciPy cuentan con la función <strong>fit</strong>. Fit nos devuelve los parámetros con los que ajusta nuestros datos al modelo. ¡Ojo, no nos avisa si es un buen ajuste o no!<br><div><em>Ejemplo: Queremos saber si la altura de los hombres adultos del pueblo de Garray sigue una distribución normal. Para ello tomamos una muestra de 80 alturas de hombres adultos en Garray.</em></div><br><div></div><br><div>Los datos los tenemos en este CSV, cada altura en una línea:</div><br><div><br><pre class="height-set:true height:200 lang:default decode:true" title="alturas.csv">Altura<br>180.55743416791302<br>159.4830930711535<br>175.54566032406794<br>149.06378740901533<br>140.35494067172635<br>146.65963134242543<br>171.34024710764376<br>140.11601629465872<br>175.6026441151595<br>158.00860559393507<br>122.53612034588375<br>116.10055909040616<br>152.89225061770068<br>148.31372767763455<br>111.17487190927599<br>160.18952563680827<br>151.8729737480722<br>141.50350042949614<br>165.2379297612276<br>150.75979657877465<br>171.5257501059296<br>157.97922034080895<br>159.60144363114716<br>152.52036681430164<br>172.0678524550487<br>163.65457704485283<br>134.9562174388093<br>189.70206097599245<br>153.78203142905076<br>176.1787894042539<br>190.83025195589502<br>199.04182673196726<br>146.97803776211907<br>174.22118528139467<br>170.95045320552694<br>161.2797407784266<br>190.61061242859464<br>168.79257731811308<br>159.87099716863165<br>136.22823975268153<br>166.87622973701335<br>179.58044852016417<br>172.49583957582817<br>165.2662334997042<br>136.6663345224381<br>161.9352364324168<br>174.56164027542448<br>161.62817356012405<br>167.65579546297906<br>170.88930983697742<br>147.22062198310996<br>151.85737964663497<br>158.03323614736198<br>135.77570282853696<br>161.25435141827515<br>193.33084953437478<br>155.43189514766172<br>155.89204074847055<br>179.23931091736836<br>146.485962651657<br>166.61617663518228<br>161.70927578953211<br>164.89798613982495<br>139.18195138901498<br>180.30341647946335<br>162.4811239647979<br>171.1035005376699<br>147.01137545913147<br>187.03282087175134<br>172.2476631392949<br>152.9814634955974<br>174.43159049461713<br>174.83877117002814<br>132.66857703218636<br>173.98029972846837<br>133.5435543737402<br>169.62941676289472<br>166.4887567852903<br>138.1150540623029<br>170.52532661450618<br></pre><br>Vamos a ajustarlo.<br><pre class="lang:python decode:true ">import pandas as pd<br>import scipy.stats as ss<br><br>df = pd.read_csv("alturas.csv")<br><br>media, desviacion = ss.norm.fit(df["Altura"])<br><br>print(media) # media = 160,37<br>print(desviacion) # desviacion = 17,41<br><br><br></pre><br>En este caso nos informa de que estos datos parecen encajar con los de una distribución normal de media 160,37 y desviación típica 17,41.<br><h2>¿Cómo de bueno es el ajuste? Kolmogorov</h2><br>Hemos hecho un ajuste, pero no sabemos qué tan bueno es. Existen varios métodos para poner a prueba los ajustes. Existen varios métodos, siendo los más populares Chi-Cuadrado y Kolmogorov-Smirnov.<br><br>Chi-Cuadrado no se puede aplicar directamente sobre distribuciones continuas, aún así voy a explicar como se haría. Sin embargo, primero vamos a probar con Kolmogorov-Smirnov, que en SciPy es <strong>ktest</strong>.<br><pre class="lang:python decode:true">import pandas as pd<br>import scipy.stats as ss<br><br>df = pd.read_csv("altura.csv")<br>media, desviacion = ss.norm.fit(df["Altura"])<br>d, pvalor = ss.ktest(df["Altura"],"norm",args=(media,desviacion))<br># o alternativamente<br>d, pvalor = ss.ktest(df["Altura"],lambda x: ss.norm.cdf(x,media,desviacion))<br></pre><br>Este test nos devuelve dos valores: D y el p-valor. Yo voy a fijarme en el p-valor. El p-valor es el nivel mínimo de significación para el cual rechazaremos el ajuste. Cuanto más cerca del 1 esté, más confianza hay en el ajuste, cuanto más cerca de cero esté, menos confianza hay en el ajuste. ¿Pero cómo se interpreta exactamente?<br><br><a href="https://files.adrianistan.eu/8ff3.jpeg"><img class="aligncenter size-large wp-image-1274" src="https://files.adrianistan.eu/8ff3-1024x844.jpeg" alt="" width="840" height="692" /></a><br><br>Una forma sencillo de entenderlo es hacer obtener el nivel de significación, que es 1 - NivelConfianza/100. Así para una confianza del 95%, este será de 0.05, para una confianza del 99%, el valor se reduce a 0.01. Si el p-valor es inferior a este nivel de significación, el ajuste probablemente no es correcto.<br><pre class="lang:python decode:true ">import pandas as pd<br>import scipy.stats as ss<br><br>df = pd.read_csv("altura.csv")<br>media, desviacion = ss.norm.fit(df["Altura"])<br>d, pvalor = ss.ktest(df["Altura"],"norm",args=(media,desviacion))<br><br># queremos confianza al 99%<br>if pvalor &lt; 0.01:<br>    print("No se ajusta a una normal")<br>else:<br>    print("Se puede ajustar a una normal")<br></pre><br>Aquí algún estadístico más serio se tiraría de los pelos, porque en realidad lo único que se puede demostrar con seguridad es en el caso de que no se ajuste. Si salta que se puede ajustar a una normal, simplemente querría decir que no se puede rechazar la hipótesis de partida (que sea normal), pero no confirma que sea normal.<br><br>Esto es algo común en el contraste de hipótesis y con los p-valores en particular y ha suscitado crítica, hasta tal punto que en 2016, la <a title="American Statistical Association" href="https://es.wikipedia.org/wiki/American_Statistical_Association">American Statistical Association</a> publicó una serie de consejos para tratar con p-valores. El p-valor en este caso solo puede demostrar que no se ajusta a una normal. No obstante, para ir poco a poco, podemos simplificar esto un poco.<br><h2>Chi-Cuadrado</h2><br>Para hacer el test de Chi-Cuadrado primero hay que dividir los datos continuos. Para uso podemos usar la función de NumPy, <strong>histogram</strong> o la de SciPy <strong>binned_statistic</strong>. Tenemos que construir una lista con las frecuencias esperadas si siguiéramos al 100% la distribución a la que queremos ajustarnos.<br><pre class="lang:python decode:true">def test_chicuadrado(data,N):<br>    n = data.count()<br>    freqs, edges, _ = ss.binned_statistic(data,data,statistic="count")<br>    def ei(i):<br>        return n*(N.cdf(edges[i])- N.cdf(edges[i-1]))<br>    expected = [ei(i) for i in range(1,len(edges))]<br>    return ss.chisquare(freqs,expected)</pre><br>Una vez tengamos una lista con los valores esperados, simplemente podemos comparar las frecuencias reales con las esperadas con <strong>chisquare</strong>. Esto nos devolverá C y el p-valor. El significado del p-valor es exactamente igual.<br><br>Con esto ya hemos visto como empezar con estadística en Python. No sé si haré más artículos de este estilo, si no es así, espero que os hayan gustado esta serie de artículos. El mundo del data science es inmenso y os invito a seguirlo explorando.<br><br></div>