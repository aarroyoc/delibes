<p>Dentro del Machine Learning existen tres ramas: aprendizaje supervisado, no supervisado y por refuerzo. En esta serie de posts vamos a hacer una introducci칩n a esta 칰ltima rama, siguiendo el esquema habitual que se usa en libros y cursos pero simplificando ciertas cosas. Este post tendr치 un caracter m치s te칩rico, pero necesario para poder desarrollar temas m치s avanzados.</p>

<div style="text-align:center">
<img width="500" src="https://files.adrianistan.eu/RLBasket.png">
</div>

<h2>쯈u칠 es el aprendizaje por refuerzo?</h2>
<p>Se trata de una rama de Machine Learning donde un agente recibe recompensas seg칰n las acciones que realice. Las recompensas, que configuramos nosotros, tienen que ir encaminadas a que el agente, al intentar conseguir la recompensa m치xima, realice lo que nosotros queremos que haga.</p>

<p>Un ejemplo. Pongamos el caso de un coche de carreras que tiene que hacer un circuito. Podemos darle un premio por cada tramo de circuito por el que pasa la primera vez, quitarle puntos si se sale, quitarle puntos si tarda mucho, ... Al final el agente que controla el coche aprender치 a que para obtener la recompensa m치xima debe pilotar el coche sin salirse haciendo todo el circuito en el menor tiempo posible.</p>

<p>A parte de las recompensas, existen otras tres diferencias respecto a las otras ramas:</p>
<ul>
<li>El feedback sobre las acciones no es instant치neo, sino que puede retrasarse.</li>
<li>El tiempo es una variable que siempre est치 presente.</li>
<li>Las acciones tomadas por el agente var칤an los datos que vamos a recibir.</li>
</ul>
<h2>T칠rminos y notaci칩n</h2>
<p>Lo primero ser치 ver los conceptos y notaciones que vamos a usar en este 치rea y que se aplicar치n posteriormente a cualquier algoritmo que encontremos.</p>

<p>La recompensa, llamada <i>R<sub>t</i>, es un valor escalar, positivo o negativo, que indica como de bien o mal est치 el agente en el instante de tiempo <i>t</i>.</p>
<p>El aprendizaje por refuerzo funciona si aceptamos la siguiente hip칩tesis como v치lida: <i>Todos los objetivos pueden describirse como la maximizaci칩n de las recompensas acumuladas  esperadas.</i>

<p>Estas recompensas se obtienen realizando acciones, <i>A<sub>t</sub></i>, es la acci칩n elegida en el instante de tiempo <i>t</i>. Estas acciones se toman de forma secuencial. Las recompensas de las acciones no tienen por qu칠 ser inmediatas. Es muy posible que sacrificar el beneficio inmediato sea mejor en pos de un beneficio mayor a largo plazo.</p>

<p>Para que el agente pueda tomar decisiones informadas, recibe una observaci칩n en cada instante de tiempo, <i>O<sub>t</sub></i>. Las observaciones dependen del problema que estemos resolviendo. Por ejemplo, en un juego arcade, las observaciones podr칤an ser una matriz de p칤xeles con los colores que tienen en ese momento en la pantalla.</p>

<p>Al conjunto de todos los paquetes Observaci칩n-Recompensa-Acci칩n hasta el instante de tiempo <i>t</i> se le denomina historia, <i>H<sub>t</sub></i>. <i>H<sub>t</sub> = O<sub>1</sub>, R<sub>1</sub>, A<sub>1</sub>, ..., A<sub>t-1</sub>, O<sub>t</sub>, R<sub>t</sub></i>. La informaci칩n que el agente usa para tomar la siguiente acci칩n se basa en la historia y se le llama estado: <i>S<sub>t</sub> = f(H<sub>t</sub>)</sub></i>. Aqu칤 hay un detalle a tener en cuenta y es que partiendo del mismo estado y tomando la misma acci칩n, el resultado (observaci칩n y recompensa siguientes) puede ser diferente.</p>

<p>Este estado tambi칠n se le llama a veces, estado del agente. Puede ocurrir que exista otro estado, el estado ambiental o del entorno: <i>S<sup>e</sup><sub>t</sub></i>. Este es el estado que tiene el entorno exterior donde vive el agente. Puede depender de variables que de cara a nuestro problema sean irrelevantes y a veces es inaccesible.</p>

<div style="text-align:center">
<img src="https://files.adrianistan.eu/RLAgent.png">
</div>

<p>Si en nuestro problema el estado del entorno es accesible, se dice que estamos ante un entorno completamente observable y en este caso, llamaremos estado indistintamente a ambos: <i>S<sub>t</sub> = S<sup>e</sup><sub>t</sub></i>. En caso contrario, hablaremos de un entorno parcialmente observable y habr치 distinci칩n, aunque lo importante sigue siendo el estado del agente. La mayor칤a de problemas en el mundo real son parcialmente observables ya que no podemos observar de forma completa el universo.</p>

<p>En aprendizaje por refuerzo vamos a trabajar con estados Markov. 쯈u칠 es eso? Son estados que contienen en s칤 mismos toda la informaci칩n hist칩rica necesaria. Es decir, una vez estamos en un estado Markov, no necesitamos mirar atr치s por qu칠 otros estados hemos pasado para hacer una buena decisi칩n ya que ser칤a redundante, el propio estado, por su definici칩n, ya contiene toda la informaci칩n para hacer esa decisi칩n. Esto quiere decir que no vamos a necesitar guardar la historia completa, ya que con guardar el estado actual, ya tenemos toda la informaci칩n. Esto en la pr치ctica significa que cualquier proceso temporal, que dependa de haber pasado antes por otros estados un n칰mero N de veces por ejemplo, va a ser representado con un estado diferente para cada valor de N.</p>
<p><i>El futuro es independiente del pasado dado el presente.</i></p>
<p>Los entornos completamente observables los podremos modelar como MDP (Markov Decission Process) mientras que los parcialmente observables los modelaremos como POMDP (Partially Observable Markov Decission Process). Gran parte del aprendizaje por refuerzo se basa en evaluar el MDP de un problema.</p>

<h3>Componentes internos de un agente</h3>
<p>En RL (reinforcement learning) un agente tiene como m칤nimo una de estas tres cosas (puede tener las tres):</p>
<ul>
<li>Pol칤tica: 쯈u칠 debe hacer el agente?</li>
<li>Funci칩n Valor: 쮺칩mo de bueno es un estado?</li>
<li>Modelo: 쮺칩mo funciona el mundo exterior?</li>
</ul>
<h4>Pol칤tica</h4>
<p>La pol칤tica de un agente es la funci칩n que indica que acci칩n tomar dado un estado (que como recordamos es Markov y es lo 칰nico necesario para tomar una buena decisi칩n). Normalmente se denomina 洧랢. <i>A<sub>t</sub> = 洧랢(S<sub>t</sub>)</i>. Las pol칤ticas pueden ser deterministas o estoc치sticas.</p>
<div style="text-align:center">
<img src="https://files.adrianistan.eu/MazeRL.png">
</div>
<p>Los algoritmos que usan solo una pol칤tica se llaman Policy Based</p>
<h4>Funci칩n Valor</h4>
<p>La funci칩n valor nos dice que tan bueno es un estado. Se trata de una predicci칩n de una recompensa futura. De este modo, podemos pensar que algunas acciones son mejores que otras en base al valor de esta funci칩n. Esta funci칩n puede adoptar muchas formas, pero lo m치s normal es estimar la funci칩n valor, dada una pol칤tica seg칰n las recompensas que ir칤a obteniendo hasta el infinito, y multiplic치ndolas por gamma (factor de descuento).</p>
<div style="text-align:center">
<img src="https://files.adrianistan.eu/FormulaFuncionValor.png">
<img src="https://files.adrianistan.eu/MazeRLValue.png">
</div>
<p>Los algoritmos que usan solo la funci칩n valor se llaman Value Based. En verdad, toda funci칩n valor lleva una pol칤tica impl칤cita pero si esta pol칤cita ya es expl칤cita e "independiente" de la funci칩n valor, estamos hablando de un algoritmo Actor Critic.</p>
<p>A parte de la funci칩n v, tambi칠n existe la funci칩n q, que es similar, pero espeficicando una acci칩n inicial.</p>
<h4>Modelo</h4>
<p>Por 칰ltimo el modelo se compondr칤a de dos partes, una parte que predice el siguiente estado del entorno (y de la misma forma, la observaci칩n que tendr칤amos) y otra que predice la recompensa.</p>
<p>Hoy en d칤a existen muchos algoritmos de RL model-free, es decir, sin modelo. Tambi칠n existen algoritmos model-based pero son menos conocidos. El problema es que muchas veces es muy costoso elaborar este tipo de modelos en la pr치ctica.</p>

<h2>Exploraci칩n vs Explotaci칩n</h2>
<p>Un agente se enfrenta a un dilema constantemente: 쯘s mejor explotar algo conocido que s칠 que funciona bien o experimento cosas nuevas para poder descurbir cosas mejores? El agente deber칤a explorar pero tampoco debe perder demasiado las recompensas que ha obtenido. Encontrar el balance adecuado es complicado.</p>
<p>Un ejemplo humano de este dilema ser칤a por ejemplo, un d칤a de cena con tus amigos, ir a un restaurante que ya conoces y que sabes que est치 bien o probar un restaurante nuevo que quiz치 sea mejor, pero no lo sabes.</p>

<h2>Predicci칩n vs Control</h2>
<p>En RL tenemos dos problemas, relacionados pero diferentes. Por un lado, la predicci칩n: evaluar correctamente el desempe침o de una pol칤tica en el futuro. Por otro lado, el control: obtener la mejor pol칤tica posible para maximizar la recompensa.</p>

<h2>Algoritmos cl치sicos</h2>
<p>Existen tres tipos de algoritmos cl치sicos para RL:</p>
<ul>
<li>Programaci칩n din치mica</li>
<li>Monte Carlo</li>
<li>Temporal Difference (TD)</li>
</ul>

<p>En el siguiente post analizaremos con m치s detalle qu칠 significa que podemos modelar un problema de RL como un MDP y qu칠 es la ecuaci칩n de Bellman.</p>

<h2>Bibliograf칤a</h2>
<ul>
<li><a href="https://www.davidsilver.uk/teaching/">Curso de David Silver de Reinforcement Learning (2015)</a></li>
<li><a href="https://torres.ai/aprendizaje-por-refuerzo">Introducci칩n al aprendizaje por refuerzo profundo de Jordi Torres</a></li>
<li><a href="http://www.incompleteideas.net/book/ebook/">Reinforcement Learning: An Introduction - Richard S. Sutton and Andrew G. Barto </a></li>
</ul>