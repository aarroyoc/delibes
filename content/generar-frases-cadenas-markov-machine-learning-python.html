Hoy vamos a hacer un ejercicio muy sencillo de machine learning. Para ello usaremos cadenas de Markov. Trataremos de generar frases totalmente nuevas basadas en otras frases que le demos como entrada.<br><br>En mi caso voy a usar las frases del presentador argentino afincado en España, <em>Héctor del Mar</em>, porque siempre me han parecido bastante graciosas y tiene unas cuantas.<br><br><a href="https://files.adrianistan.eu/hectordelmar.jpg"><img class="wp-image-1293 size-full" src="https://files.adrianistan.eu/hectordelmar.jpg" alt="" width="640" height="480" /></a> Héctor del Mar es el de la derecha. Para quien no le conozca, suele comentar los shows de la WWE<br><h2>¿Qué son las cadenas de Markov?</h2><br>Las <a href="https://es.wikipedia.org/wiki/Cadena_de_M%C3%A1rkov">cadenas de Markov</a> es un modelo probabilístico que impone que la probabilidad de que suceda algo solo depende del estado anterior. Aplicado a nuestro caso con palabras, la probabilidad de que una palabra sea la siguiente de la frase solo depende de la palabra anterior. Observemos este grafo:<br><br><a href="https://files.adrianistan.eu/Markov.png"><img class="aligncenter size-large wp-image-1294" src="https://files.adrianistan.eu/Markov-1024x768.png" alt="" width="840" height="630" /></a>En él se han introducido dos frases: <em>El coche rojo </em>y <em>El coche verde</em>. La probabilidad de que <em>coche </em>sea la palabra que va después de <em>El</em> es del 100%, pero de que <em>rojo</em> sea la siguiente palabra después de <em>coche</em> es del 50%. Con este ejemplo, parece bastante limitado, pero la cosa cambia cuando metemos muchas frases y muchas palabras se repiten.<br><br>Para este ejemplo no obstante, usaré las dos últimas palabras como estado anterior, ya que suele dar resultados mucho más legibles (aunque pueden darse con más probabilidad frases que ya existen).<br><h2>Obteniendo los datos</h2><br>El primer paso será tener las frases en un formato óptimo. Para ello usaré <strong>requests</strong> y <strong>BeautifulSoup</strong><strong>4</strong>. Las frases las voy a sacar de <a href="https://es.wikiquote.org/wiki/H%C3%A9ctor_del_Mar">Wikiquote</a>.<br><pre class="lang:python decode:true ">from bs4 import BeautifulSoup<br>import requests<br><br>r = requests.get("https://es.wikiquote.org/wiki/H%C3%A9ctor_del_Mar")<br>soup = BeautifulSoup(r.text,"lxml")<br>frases = map(lambda x: x.text.replace("\"",""),soup.select(".mw-parser-output li"))<br>palabras = map(lambda x: str(x).split(" "),frases)</pre><br><h2>Generando el grafo de Markov</h2><br>Ahora hay que generar el grafo de Markov. Para ello vamos a usar un diccionario, donde en la clave tendremos el estado anterior, es decir, las dos últimas palabras (en forma de tupla). El contenido será una lista con todas las palabras a las que puede saltar. Al ser una lista, puede haber palabras repetidas, lo que efectivamente hará aumentar su probabilidad.<br><br>Aprovechando Python, voy a usar un <strong>defaultdict</strong> para simplificar el código, ya que con él me voy a asegurar de que todos los accesos al diccionario me van a devolver una lista.<br><pre class="lang:python decode:true ">from collections import defaultdict<br><br>almacen = defaultdict(lambda: list())<br><br>def add_word(prev,next):<br>    global almacen<br>    almacen[prev].append(next)<br><br>for frase in palabras:<br>    frase = list(frase)<br>    for i,palabra in enumerate(frase):<br>        if i == 0:<br>            add_word(("START","START"),palabra)<br>            continue<br>        if i == 1:<br>            add_word(("START",frase[0]),palabra)<br>            continue<br>        add_word((frase[i-2],frase[i-1]),palabra)</pre><br><h2>Generando una frase nueva</h2><br>Ahora viene el último paso, generrar una frase nueva. Para ello, empezamos con el estado <em>START,START</em> y seguimos el grafo hasta que acabemos. Para elegir la siguiente palabra de la lista usamos <strong>random.choice</strong>. La frase que se va generando se queda en una lista hasta que finalmente devolvemos un string completo.<br><pre class="lang:python decode:true">import random<br><br>def gen_word():<br>    word = list()<br>    state = "START","START"<br>    while True:<br>        w = random.choice(almacen[state])<br>        word.append(w)<br>        state = state[1],w<br>        if w.endswith(".") or w.endswith("!"):<br>            return " ".join(word)<br></pre><br><h2>Resultados</h2><br>Veamos los resultados:<br><br><a href="https://files.adrianistan.eu/MarkovHectorDelMar.png"><img class="aligncenter size-large wp-image-1295" src="https://files.adrianistan.eu/MarkovHectorDelMar-1024x768.png" alt="" width="840" height="630" /></a>Las frases en rojo son frases que dijo de verdad. Las frases en verde son frases generadas por este machine learning. La tasa de frases nuevas no es muy elevada, pero son más del 50%. Y todas son bastante divertidas.<br><br>El código fuente completo de <em>Markov-HectorDelMar</em> está en el repositorio Git del blog: <a href="https://github.com/aarroyoc/blog-ejemplos/tree/master/markov-hector-del-mar">https://github.com/aarroyoc/blog-ejemplos/tree/master/markov-hector-del-mar</a><br><br>Ahora que ya sabes usar cadenas de Markov, ¿por qué no meter como dato de entrada <em>El Quijote?</em>, ¿o los tuits de algún político famoso?, ¿o las entradas de este blog? Las posibilidades son infinitas.<br><br>Para despedirme, como diría el Héctor del Mar de verdad:<br><blockquote>Aquí estoy porque he venido, porque he venido aquí estoy, si no le gusta mi canto, como he venido, me voy. ¡Nos vamos, don Fernando!</blockquote>