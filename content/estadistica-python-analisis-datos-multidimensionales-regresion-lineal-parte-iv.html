Hasta ahora hemos tratado con una única variable por separado. Ahora vamos a ver qué podemos hacer con varias variables en la misma muestra (conjunto de datos). Nos interesará saber si están relacionadas o no (<strong>independencia</strong> o no). Si existe relación (estan <strong>correlacionadas</strong>) vamos a construir un modelo de <strong>regresión lineal</strong>.<br><h2>Distribución conjunta de frecuencias</h2><br>En el caso de dos variables, podemos construir una distribución conjunta de frecuencias. Se trata de una tabla de doble entrada donde cada dimensión corresponde a cada variable y el valor de las celdas representa la frecuencia del par. Para ello podemos usar <strong>crosstab</strong> también (de hecho, su uso original es este).<br><br><em>Ejemplo: En las votaciones a alcalde de la ciudad de Valladolid se presentaban Rafael, Silvia y Olga. Analiza los resultados e informa de quién fue el ganador de las elecciones. ¿Quién fue el candidato favorito en el barrio de La Rondilla?</em><br><pre class="lang:default decode:true " title="votos.csv">usuario,voto,distrito<br>Nerea,Rafael,Centro<br>Esteban,Olga,Centro<br>Ismael,Silvia,Centro<br>Silvia,Rafael,Centro<br>Susana,Rafael,Centro<br>Laura,Rafael,Centro<br>Raquel,Olga,Centro<br>Eduardo,Olga,La Rondilla<br>Javier,Silvia,La Rondilla<br>Saturnino,Rafael,La Rondilla<br>Segundo,Olga,La Rondilla<br>Celia,Silvia,La Rondilla<br>Olga,Rafael,La Rondilla<br>Casimiro,Silvia,La Rondilla<br>Rafael,Silvia,La Rondilla</pre><br><pre class="lang:python decode:true">import pandas as pd<br><br>df = pd.read_csv("votos.csv")<br>tab = pd.crosstab(df["voto"],df["distrito"],margins=True) # margins si queremos generar la fila y columna All<br>total = tab["All"][:-1]<br>winner = total.idxmax()<br>print("El ganador de las elecciones fue "+str(winner))<br><br>rondilla = tab["La Rondilla"][:-1]<br>winner_rondilla = rondilla.idxmax()<br>print("El ganador en el distrito de La Rondilla fue "+str(winner_rondilla))</pre><br><a href="https://files.adrianistan.eu/VotosCrosstab.png"><img class="aligncenter size-full wp-image-1150" src="https://files.adrianistan.eu/VotosCrosstab.png" alt="" width="652" height="460" /></a>Como podéis ver, un humando podría haber sacado estas conclusiones observando simplemente la tabla conjunta de frecuencias. ¿Quién tiene más votos en total? Rafael, con 6 en All (la suma de los distritos). ¿Quién ha sacado más votos en La Rondilla? Silvia, con 4 en la columna de La Rondilla. Por último, ¿votó más gente en el Centro o en La Rondilla? Votaron más en La Rondilla (8 votos), que en el Centro (7 votos).<br><br>A las frecuencias <strong>All</strong> se las llama comúnmente <strong>distribuciones marginales</strong>. Cuando discriminamos las frecuencias a un solo valor de una variable, se habla de <strong>distribuciones condicionadas</strong>, en este caso hemos usado la distribución de votos condicionada al distrito La Rondilla. Estas distribuciones son univariantes como habréis sospechado.<br><h2>Gráfico XY o bivariante</h2><br>Una manera muy útil de observar posibles correlaciones es con el gráfico XY, solamente disponible para distribuciones bivariantes. Cada observación se representa en el plano como un punto. En Matplotlib podemos dibujarlo con <strong>scatter</strong>.<br><br><em>Ejemplo: Represente el gráfico XY de las variables ingresos y gastos de las familias.</em><br><pre class="lang:default decode:true " title="ingresos_gastos.csv">id,ingresos,gastos<br>1,1000,800<br>2,1100,885<br>3,1100,880<br>4,1200,1000<br>5,1400,1000<br>6,900,750<br>7,600,550<br>8,2000,1200<br>9,1200,1000<br>10,1250,1000<br>11,3000,2400<br>12,2200,1700<br>13,1700,1300<br>14,1650,1220<br>15,1825,1500<br>16,1435,1200<br>17,980,800<br>18,1050,800<br>19,2105,1680<br>20,1280,1020<br>21,1590,1272<br>22,1300,1040<br>23,1200,1000<br>24,1110,890<br>25,850,680</pre><br><pre class="lang:python decode:true">import pandas as pd<br>import matplotlib.pyplot as plt<br><br>df = pd.read_csv("ingresos_gastos.csv")<br>plt.scatter(df["ingresos"],df["gastos"])<br>plt.xlabel("Ingresos")<br>plt.ylabel("Gastos")<br>plt.show()</pre><br><a href="https://files.adrianistan.eu/IngresosGastos.png"><img class="aligncenter size-full wp-image-1151" src="https://files.adrianistan.eu/IngresosGastos.png" alt="" width="640" height="480" /></a>En la imagen podemos ver cada dato representado por un punto. En este ejemplo puede apreciarse como los puntos estan en torno a una línea recta invisible.<br><h2>Covarianza</h2><br>Para medir la relación entre dos variables podemos definir la covarianza:<br><p style="text-align: center;">[latex]<br>cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N}<br>[/latex]</p><br>Pandas trae el método <strong>cov</strong> para calcular la matriz de covarianzas. De esta matriz, obtendremos el valor que nos interesa.<br><pre class="lang:python decode:true ">import pandas as pd<br><br>df = pd.read_csv("ingresos_gastos.csv")<br><br>covarianza = df.cov()["ingresos"]["gastos"]<br></pre><br>¿Y la covarianza qué nos dice? Por si mismo, bastante poco. Como mucho, si es positivo nos dice que se relacionarían de forma directa y si es negativa de forma inversa. Pero la covarianza está presente en muchas fórmulas.<br><h2>Coeficiente de correlación lineal de Pearson</h2><br><p style="text-align: center;">[latex]<br>r_{x,y}=\frac{cov_{x,y}}{s_{x}s_{y}}<br>[/latex]</p><br>Uno de los métodos que usa la covarianza (aunque Pandas lo va a hacer solo) es el coeficiente de correlación lineal de Pearson. Cuanto más se acerque a 1 o -1 más correlacionadas están las variables. Su uso en Pandas es muy similar a la covarianza.<br><pre class="lang:default decode:true ">import pandas as pd<br><br>df = pd.read_csv("ingresos_gastos.csv")<br>r = df.corr(method="pearson")["ingresos"]["gastos"]</pre><br>En este ejemplo concreto, el coeficiente de correlación de Pearson nos da <strong>0.976175</strong>. Se trata de un valor lo suficientemente alto como para plantearnos una correlación lineal. Es decir, que pueda ser aproximado por una recta. Si este coeficiente es igual a 1 o -1, se puede decir que una variable es fruto de una transformación lineal de la otra.<br><h2>Ajuste lineal</h2><br>Vamos a intentar encontrar un modelo lineal que se ajuste a nuestras observaciones y nos permita hacer predicciones. Esta recta se llamará <strong>recta de regresión</strong> y se calcula de la siguiente forma:<br><p style="text-align: center;">[latex]<br>\hat{y}-\bar{y}=\frac{cov_{x,y}}{s_{x}^2}(x-\bar{x})<br>[/latex]</p><br>Usando las funciones de varianza, media y covarianza Pandas no es muy complicado hacer una recta de regresión:<br><pre class="lang:python decode:true">def recta(x):<br>    pendiente = df.cov()["ingresos"]["gastos"]/df["ingresos"].var()<br>    return pendiente*(x-df["ingresos"].mean())+df["gastos"].mean()</pre><br>Que podemos probar visualmente:<br><pre class="lang:python decode:true">import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br><br>df = pd.read_csv("ingresos_gastos.csv")<br><br>def recta(x):<br>    pendiente = df.cov()["ingresos"]["gastos"]/df["ingresos"].var()<br>    return pendiente*(x-df["ingresos"].mean())+df["gastos"].mean()<br><br>line = [recta(x) for x in np.arange(3000)]<br>plt.scatter(df["ingresos"],df["gastos"])<br>plt.plot(line)<br>plt.show()</pre><br><a href="https://files.adrianistan.eu/Correlacion.png"><img class="aligncenter size-full wp-image-1156" src="https://files.adrianistan.eu/Correlacion.png" alt="" width="640" height="480" /></a>Sin embargo SciPy ya nos trae un método que calcula la pendiente, la ordenada en el origen, el coeficiente de correlación lineal de Pearson y mucho más en un solo lugar. Es mucho más eficiente, se trata de <strong>linregress</strong>.<br><pre class="lang:python decode:true">import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br>from scipy import stats as ss<br><br>df = pd.read_csv("ingresos_gastos.csv")<br>pendiente, ordenada, pearson, p, error = ss.linregress(df["ingresos"],df["gastos"])<br><br>def recta(x):<br>    return pendiente*x + ordenada<br><br>recta = np.vectorize(recta)<br>linea = recta(np.arange(3000))<br>plt.scatter(df["ingresos"],df["gastos"])<br>plt.plot(linea)<br>plt.show()</pre><br>Además, para calcular los valores del gráfico, he usado <strong>vectorize</strong> de NumPy, que permite mapear los arrays nativos de NumPy. Más eficiente. Mismo resultado.<br><h2>La ley de Ohm</h2><br><a href="https://files.adrianistan.eu/Georg_Simon_Ohm3.jpg"><img class="aligncenter size-full wp-image-1157" src="https://files.adrianistan.eu/Georg_Simon_Ohm3.jpg" alt="" width="337" height="428" /></a>¿Quién no ha oído hablar de la Ley de Ohm? Se trata de una ley que relaciona la diferencia de potencial con el amperaje dando lugar a la resistencia. La ley fue enunciada por George Simon Ohm, aunque no exactamente como la conocemos hoy en día. En este ejemplo vamos a deducir de cero la ley de Ohm. <em>Este ejercicio se puede hacer en casa con datos reales si se dispone de un polímetro (dos mejor) y una fuente de alimentación con tensión regulable. Este ejercicio pueden hacerlo niños sin problema.</em><br><br><strong>Olvida todo lo que sepas de la ley de Ohm</strong><br><br>Es posible apreciar que en un circuito con una bombilla, si introducimos una pieza cerámica, la intensidad de la bombilla disminuye.<br><br><a href="https://files.adrianistan.eu/Bodacious-Borwo-Blorr-1.png"><img class="wp-image-1159 size-large" src="https://files.adrianistan.eu/Bodacious-Borwo-Blorr-1-1024x419.png" alt="" width="840" height="344" /></a> Cuando la corriente no atraviesa la resistencia<br><br><a href="https://files.adrianistan.eu/Bodacious-Borwo-Blorr-2.png"><img class="size-large wp-image-1160" src="https://files.adrianistan.eu/Bodacious-Borwo-Blorr-2-1024x419.png" alt="" width="840" height="344" /></a> Cuando la corriente atraviesa la resistencia<br><br>¿Qué ocurre exactamente? ¿Por qué la bombilla tiene menos intensidad en el segundo caso?<br><br>Vamos a aislar la resistencia. Ponemos un voltímetro y un amperímetro y vamos cambiando la tensión de entrada. Anotamos la corriente medida en cada caso.<br><br><a href="https://files.adrianistan.eu/Bodacious-Borwo-Blorr-3.png"><img class="aligncenter size-large wp-image-1161" src="https://files.adrianistan.eu/Bodacious-Borwo-Blorr-3-1024x419.png" alt="" width="840" height="344" /></a><br><pre class="lang:default decode:true" title="voltaje_intensidad.csv">voltaje,intensidad<br>5,1.01<br>6,1.19<br>7,1.4<br>8,1.62<br>9,1.81<br>10,2.01</pre><br>Podemos intentar hacer un ajuste lineal a estos datos. De modo, que una vez sepamos la intensidad, podamos predecir el voltaje.<br><pre class="lang:python decode:true">import pandas as pd<br>from scipy import stats as ss<br><br>df = pd.read_csv("voltaje_intensidad.csv")<br>pendiente, ordenada_origen, pearson, p, error = ss.linregress(df["intensidad"],df["voltaje"])<br><br>print(pearson) # 0.99969158942375369 es un valor que indica una muy fuerte correlación lineal<br></pre><br>Como Pearson nos da un número muy próximo a 1, podemos definir un <strong>modelo matemático</strong> siguiendo la <strong>regresión lineal</strong>.<br><br>Este modelo matemático se define así:<br><pre class="lang:python decode:true">def voltaje(intensidad):<br>    return intensidad*pendiente + ordenada_origen</pre><br>Y es lo que se conoce como la <strong>Ley de Ohm</strong>. En realidad, la ordenada en el origen tiene un valor muy cercano a cero, podemos quitarlo.<br><br>Así nos queda un modelo capaz de predecir lel voltaje en base a la intensidad y la pendiente de la recta. Ahora puedes probar cambiando la resistencia y observando que siempre ocurre lo mismo. Tenemos el <strong>voltaje</strong> por la <strong>pendiente</strong> del modelo. Este valor de la pendiente es lo que en física se llama <strong>resistencia</strong> y se mide en <strong>ohmios</strong>. Así se nos queda entonces la ley de Ohm que todos conocemos:<br><p style="text-align: center;">[latex]<br>V= IR<br>[/latex]</p><br>En este caso la pendiente equivalía a <strong>4.94 Ω</strong>, valor muy cercano a los <strong>5 Ω</strong> que dice el fabricante.